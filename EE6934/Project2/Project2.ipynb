{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2osEUslUGkv"
   },
   "source": [
    "# CEG5304/EE6934 Deep Learning Project 2\n",
    "\n",
    "In this project, you are going to start an exciting journey to explore Deep Learning and Neural Networks by completing the following three tasks:\n",
    "\n",
    "* Task 1. Building Neural Network (30%)\n",
    "* Task 2. Exploring Model Training (30%)\n",
    "* Task 3. Adversarial Attack (40%)\n",
    "\n",
    "Before doing the project, please read the instructions carefully (failure to do so will be penalized):\n",
    "\n",
    "1. Implement your codes **within** \"TODO\" and \"END OF YOUR CODE\", do **NOT** modify any codes outside the answer area;\n",
    "2. Make sure your codes **clean**, **easily readable** (add meaningful comments if needed), and **runnable**;\n",
    "3. Write your answers in the given markdown cells, keep your answers clear and concise;\n",
    "4. Do submit your project to \"Assignments --> Project 2\" on [Canvas](https://canvas.nus.edu.sg/) before the deadline: **5:59 pm (SGT), 6 March, 2023**;\n",
    "5. This is an individual project, do **NOT** share your solutions with others, we have zero tolerance for cheating.\n",
    "\n",
    "If you have any questions regarding this project, please feel free to contact Zhangjie Wu (zhangjiewu@u.nus.edu) and Liu, Jiawei (jiawei.liu@u.nus.edu). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyJICDR5tkuj"
   },
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2HVv6XcoTjb"
   },
   "source": [
    "### Loading packages\n",
    "\n",
    "Please install the packages listed below if you haven't done so. To avoid unnecessary trouble reproducing your assignment, we recommend you to use `torch==1.9.0` and `torchvision==0.10.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Mar  4 13:41:53 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.05    Driver Version: 525.85.05    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   44C    P4    22W / 175W |   2084MiB / 16376MiB |     50%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2441      G   /usr/lib/xorg/Xorg               1581MiB |\n",
      "|    0   N/A  N/A      2680      G   /usr/bin/gnome-shell              224MiB |\n",
      "|    0   N/A  N/A      3392      G   ...223603882393556080,131072      207MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch==1.9.0\n",
    "# !pip install torchvision==0.10.0\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LNFcTvFWg1L1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.cm as mpl_color_map\n",
    "from PIL import Image\n",
    "from random import randint\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Owj6jKvWraFJ"
   },
   "source": [
    "### Checking the Running Device\n",
    "\n",
    "It is recommended to run this notebook on GPUs since that would be a much faster way to train and evaluate the model. If you have difficulties accessing GPUs with your machine, you can choose to use the free GPUs provided by [Google Colab](https://colab.research.google.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CLovzR7Lk49o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = 'cuda:0'\n",
    "else:\n",
    "  device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fb12tK49rDia"
   },
   "source": [
    "### Getting the CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "EIT2XbCfp2iv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "CIFAR_CLASSES = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "CIFAR_MEAN, CIFAR_STD = np.array([0.4914, 0.4822, 0.4465]), np.array([0.247, 0.243, 0.261])\n",
    "\n",
    "# Convert image to pytroch tensor and normalize\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=CIFAR_MEAN.tolist(), std=CIFAR_STD.tolist())\n",
    "])\n",
    "\n",
    "# Inverse operation to regain original image\n",
    "inverse_transform = torchvision.transforms.Compose([ \n",
    "    torchvision.transforms.Normalize(mean=[0, 0, 0], std=(1 / CIFAR_STD).tolist()),\n",
    "    torchvision.transforms.Normalize(mean=(-CIFAR_MEAN).tolist(), std=[1, 1, 1]),\n",
    "    torchvision.transforms.ToPILImage()\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "train_set = torchvision.datasets.CIFAR10(root=\"data\", train=True, download=True, transform=transform)\n",
    "train_size = len(train_set)\n",
    "test_set = torchvision.datasets.CIFAR10(root=\"data\", train=False, download=True, transform=transform)\n",
    "test_size = len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sbRn41AoTje"
   },
   "source": [
    "### Helper Fuctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "byI3kUUeoTje"
   },
   "outputs": [],
   "source": [
    "def show_prob_cifar(image, label, p):\n",
    "    \"\"\"\n",
    "        Show image and prediction probability for CIFAR-10 dataset.\n",
    "    Args:\n",
    "    Inputs\n",
    "        image (Tensor): Input image\n",
    "        label (int): The ground truth label of input image\n",
    "        p (Tensor): Class probability of input image\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    ft = 10\n",
    "    width = 0.9\n",
    "    col = 'blue'\n",
    "\n",
    "    p=p.cpu().data.squeeze().numpy()\n",
    "    y_pos = np.arange(len(p))\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "    # Plot image\n",
    "    ax1 = plt.subplot(2, 1, 1)\n",
    "    ax1.imshow(image)\n",
    "    ax1.set_axis_off()\n",
    "    ax1.set_title('Ground Truth: ' + CIFAR_CLASSES[label])\n",
    "\n",
    "    # Plot probability\n",
    "    ax2 = plt.subplot(2, 1, 2)\n",
    "    ax2.barh(y_pos, p*0.1, width , align='center', color=col)\n",
    "    ax2.set_yticks(y_pos)\n",
    "    ax2.set_yticklabels(CIFAR_CLASSES, fontsize=ft)\n",
    "    ax2.invert_yaxis()  \n",
    "    ax2.set_xticklabels([])\n",
    "    ax2.set_xticks([])\n",
    "    ax2.spines['right'].set_visible(False)\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.spines['bottom'].set_visible(False)\n",
    "    ax2.spines['left'].set_linewidth(4)\n",
    "    for i in range(len(p)):\n",
    "        str_nb=\"{0:.2f}\".format(p[i])\n",
    "        ax2.text(p[i]*0.1 + 0.001, y_pos[i] ,str_nb ,\n",
    "                 horizontalalignment='left', verticalalignment='center',\n",
    "                 transform=ax2.transData, color= col,fontsize=ft)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def apply_colormap_on_image(org_im, activation, colormap_name):\n",
    "    \"\"\"\n",
    "        Apply heatmap on image\n",
    "    Args:\n",
    "        org_img (PIL img): Original image\n",
    "        activation_map (numpy arr): Activation map (grayscale) 0-255\n",
    "        colormap_name (str): Name of the colormap\n",
    "    \"\"\"\n",
    "    # Get colormap\n",
    "    color_map = mpl_color_map.get_cmap(colormap_name)\n",
    "    no_trans_heatmap = color_map(activation)\n",
    "    # Change alpha channel in colormap to make sure original image is displayed\n",
    "    heatmap = copy.copy(no_trans_heatmap)\n",
    "    heatmap[:, :, 3] = 0.4\n",
    "    heatmap = Image.fromarray((heatmap*255).astype(np.uint8))\n",
    "    no_trans_heatmap = Image.fromarray((no_trans_heatmap*255).astype(np.uint8))\n",
    "\n",
    "    # Apply heatmap on iamge\n",
    "    heatmap_on_image = Image.new(\"RGBA\", org_im.size)\n",
    "    heatmap_on_image = Image.alpha_composite(heatmap_on_image, org_im.convert('RGBA'))\n",
    "    heatmap_on_image = Image.alpha_composite(heatmap_on_image, heatmap)\n",
    "    return no_trans_heatmap, heatmap_on_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzXv8qM1rPlc"
   },
   "source": [
    "## Task 1: Building Neural Network\n",
    "\n",
    "In this task, you are expected to build a convolutional neural network (CNN) for solving a problem of image recognition and explore strategies that could further strengthen its performance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amSbmHCa84Yf"
   },
   "source": [
    "### Defining the Model\n",
    "\n",
    "In deep learning, a Convolutional Neural Network (CNN, or ConvNet) is one of the most famous deep learning models that have been widely used in the field of computer vision. A simple CNN often consists of three main types of layers:\n",
    "- **Convolutional Layer**, the core building block that takes over the most of computational burdens, contains a set of filters (or kernels) with learnable parameters while training. It receives as input an image (or a feature map), and computes its output volume by stacking the activation maps convolved by every filter along the depth dimension. \n",
    "- **Pooling Layer** is commonly inserted in-between successive Conv layers in a CNN architecture. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to also control overfitting. \n",
    "- **Fully Connected Layer** have fully connections to all activations in the previous layer and is usually placed before the output layer to form the last few layers of a CNN Architecture.\n",
    "\n",
    "If you are not familiar with CNN architectures, [this blog](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53) may help you get a better understanding of the mechanism of each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rm2a5m_WoTjf"
   },
   "source": [
    "Define a naive CNN stacked by the layers mentioned above. While doing so, please follow the TODOs provided below. (Refer to [torch.nn](https://pytorch.org/docs/stable/nn.html) for detailed instructions of building basic blocks with PyTorch.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LVv2SxOihl6K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "  (fc3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (fc4): Linear(in_features=4096, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        ##############################################################################\n",
    "        # TODO: Define a simple CNN contraining Conv, Pooling, and FC layers.        #\n",
    "        ##############################################################################\n",
    "        \n",
    "        # Block 1:         3 x 32 x 32 --> 32 x 16 x 16     \n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3,padding=1)\n",
    "        self.pool1 = torch.nn.MaxPool2d(2,2)\n",
    "        # Block 2:         32 x 16 x 16 --> 64 x 8 x 8\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,padding=1)\n",
    "        self.pool2 = torch.nn.MaxPool2d(2,2)\n",
    "        # Block 3:         64 x 8 x 8 --> 128 x 4 x 4        \n",
    "        self.conv3 = torch.nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,padding=1)\n",
    "        self.pool3 = torch.nn.MaxPool2d(2,2)\n",
    "        # Block 4:          128 x 4 x 4 --> 256 x 2 x 2\n",
    "        self.conv4 = torch.nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3,padding=1)\n",
    "        self.pool4 = torch.nn.MaxPool2d(2,2)\n",
    "        # Linear layers:   256 x 2 x 2 --> 1024 --> 2048 --> 4096 --> 4096 --> 10\n",
    "        self.fc1 = torch.nn.Linear(1024,2048)\n",
    "        self.fc2 = torch.nn.Linear(2048,4096)\n",
    "        self.fc3 = torch.nn.Linear(4096,4096)\n",
    "        self.fc4 = torch.nn.Linear(4096,10)\n",
    "        ##############################################################################\n",
    "        #                             END OF YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        ##############################################################################\n",
    "        # TODO: Implement forward path turning an input image to class probability.  #\n",
    "        # For activation function, please use ReLU.                                  #\n",
    "        ##############################################################################\n",
    "\n",
    "        # Block 1:         3 x 32 x 32 --> 32 x 16 x 16\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        # Block 2:         32 x 16 x 16 --> 64 x 8 x 8\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        # Block 3:         64 x 8 x 8 --> 128 x 4 x 4\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        # Block 4:         128 x 4 x 4 --> 256 x 2 x 2\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool4(x)\n",
    "        # Linear layers:   256 x 2 x 2 --> 1024 --> 2048 --> 4096 --> 4096 --> 10\n",
    "        x = torch.flatten(x,1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        ##############################################################################\n",
    "        #                             END OF YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = ConvNet()\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CT0GtJL3Ag-I"
   },
   "source": [
    "### Training the Model\n",
    "\n",
    "Some default settings for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jgNLaSYeoTjf"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 20\n",
    "batch_size = 128\n",
    "learning_rate = 0.25\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# Define loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# Build data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "data_loaders = {\"train\": train_loader, \"test\": test_loader}\n",
    "dataset_sizes = {\"train\": train_size, \"test\": test_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5IIzOnnoTjf"
   },
   "source": [
    "Here, your job is to implement two functions for training and testing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "i8ERy2xSuygX"
   },
   "outputs": [],
   "source": [
    "def eval_on_test_set(model):\n",
    "    model.eval()\n",
    "    running_error = 0\n",
    "\n",
    "    for data in test_loader:\n",
    "        pass\n",
    "        ##############################################################################\n",
    "        # TODO: Implement the evaluation process on test set.                        #\n",
    "        ##############################################################################\n",
    "\n",
    "        # Load inputs and labels and deploy to running device\n",
    "        tmp_img, tmp_label = data\n",
    "        tmp_img, tmp_label = tmp_img.cuda(), tmp_label.cuda()\n",
    "\n",
    "        # Forward batch data through the net\n",
    "        tmp_output = model(tmp_img)\n",
    "        _, tmp_predicted = torch.max(tmp_output, 1)\n",
    "\n",
    "        # Compute the error made on this batch and add it to the running error\n",
    "        running_error += (tmp_predicted != tmp_label).sum().item()\n",
    "\n",
    "        ##############################################################################\n",
    "        #                             END OF YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "        \n",
    "    total_error = running_error / test_size\n",
    "    print('error rate on test set = {:.2f}%'.format(total_error * 100))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "iL3QYrp7u1V4"
   },
   "outputs": [],
   "source": [
    "def train_net(model):\n",
    "    start=time.time()\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # set the running quatities to zero at the beginning of the epoch\n",
    "        running_loss = 0\n",
    "        running_error = 0\n",
    "    \n",
    "        for data in train_loader:\n",
    "            pass\n",
    "            ##############################################################################\n",
    "            # TODO: Implement the training process.                                      #\n",
    "            ##############################################################################\n",
    "\n",
    "            # Load inputs and labels and deploy to running device\n",
    "            tmp_img, tmp_label = data\n",
    "            tmp_img, tmp_label = tmp_img.cuda(), tmp_label.cuda()\n",
    "\n",
    "            # Set the gradients to zeros\n",
    "            optimizer.zero_grad()\n",
    "            # Forward the batch data through the net\n",
    "            tmp_output = model(tmp_img)    \n",
    "\n",
    "            # Compute the average of the losses of the data points in the minibatch\n",
    "            tmp_loss = criterion(tmp_output, tmp_label)\n",
    "\n",
    "            # Backward pass to compute gradients\n",
    "            tmp_loss.backward()\n",
    "\n",
    "            # Do one step of stochastic gradient descent\n",
    "            optimizer.step()\n",
    "\n",
    "            # Add the loss of this batch to the running loss\n",
    "            running_loss += tmp_loss.item()\n",
    "\n",
    "            # Compute the error made on this batch and add it to the running error\n",
    "            _, tmp_predicted = torch.max(tmp_output, 1)\n",
    "            running_error += (tmp_predicted != tmp_label).sum().item()\n",
    "            \n",
    "            ##############################################################################\n",
    "            #                             END OF YOUR CODE                               #\n",
    "            ##############################################################################\n",
    "            \n",
    "        # Compute stats for the full training set\n",
    "        total_loss = running_loss / train_size\n",
    "        total_error = running_error / train_size\n",
    "        elapsed = (time.time()-start) / 60\n",
    "        \n",
    "        print('epoch= {} \\t time= {:.2f} min \\t loss= {:.3f} \\t error= {:.2f}%'.format(epoch, elapsed, total_loss, total_error * 100))\n",
    "        eval_on_test_set(model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ITE1NSfoTjg"
   },
   "source": [
    "Run `train_net` and start training. After training, your error on the testing set should be under 30% (if not, please look back and check your codes, there might be something wrong with the network architecture or training process). Once the training phrase is completed, save the trained model on your device so it can be directly loaded in the following tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qZPoh-WQoTjg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0 \t time= 0.07 min \t loss= 0.016 \t error= 76.98%\n",
      "error rate on test set = 65.68%\n",
      "epoch= 1 \t time= 0.12 min \t loss= 0.012 \t error= 58.40%\n",
      "error rate on test set = 49.49%\n",
      "epoch= 2 \t time= 0.17 min \t loss= 0.010 \t error= 45.44%\n",
      "error rate on test set = 46.49%\n",
      "epoch= 3 \t time= 0.22 min \t loss= 0.008 \t error= 36.44%\n",
      "error rate on test set = 43.30%\n",
      "epoch= 4 \t time= 0.27 min \t loss= 0.007 \t error= 29.66%\n",
      "error rate on test set = 29.87%\n",
      "epoch= 5 \t time= 0.36 min \t loss= 0.006 \t error= 25.14%\n",
      "error rate on test set = 30.93%\n",
      "epoch= 6 \t time= 0.46 min \t loss= 0.005 \t error= 20.46%\n",
      "error rate on test set = 29.43%\n",
      "epoch= 7 \t time= 0.57 min \t loss= 0.004 \t error= 17.00%\n",
      "error rate on test set = 27.61%\n",
      "epoch= 8 \t time= 0.67 min \t loss= 0.003 \t error= 13.52%\n",
      "error rate on test set = 28.93%\n",
      "epoch= 9 \t time= 0.78 min \t loss= 0.002 \t error= 10.62%\n",
      "error rate on test set = 27.49%\n",
      "epoch= 10 \t time= 0.88 min \t loss= 0.002 \t error= 8.36%\n",
      "error rate on test set = 27.46%\n",
      "epoch= 11 \t time= 0.99 min \t loss= 0.001 \t error= 6.38%\n",
      "error rate on test set = 27.43%\n",
      "epoch= 12 \t time= 1.10 min \t loss= 0.001 \t error= 5.36%\n",
      "error rate on test set = 26.76%\n",
      "epoch= 13 \t time= 1.21 min \t loss= 0.001 \t error= 4.28%\n",
      "error rate on test set = 26.52%\n",
      "epoch= 14 \t time= 1.31 min \t loss= 0.001 \t error= 3.59%\n",
      "error rate on test set = 26.45%\n",
      "epoch= 15 \t time= 1.42 min \t loss= 0.001 \t error= 2.91%\n",
      "error rate on test set = 25.87%\n",
      "epoch= 16 \t time= 1.53 min \t loss= 0.001 \t error= 2.71%\n",
      "error rate on test set = 26.32%\n",
      "epoch= 17 \t time= 1.64 min \t loss= 0.001 \t error= 2.34%\n",
      "error rate on test set = 29.52%\n",
      "epoch= 18 \t time= 1.74 min \t loss= 0.001 \t error= 2.17%\n",
      "error rate on test set = 26.88%\n",
      "epoch= 19 \t time= 1.84 min \t loss= 0.000 \t error= 1.51%\n",
      "error rate on test set = 24.97%\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "train_net(model)\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), './model_cnn.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D94cT95fDn4q"
   },
   "source": [
    "### Ploting the Results\n",
    "\n",
    "Now you have finished model training, let's randomly pick some test images, feed them into your model and see how they turn out. (There is nothing to implement in this section.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "OAEMjcgax1cK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "  (fc2): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "  (fc3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (fc4): Linear(in_features=4096, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert os.path.exists('./model_cnn.pt'), 'train the model first'\n",
    "# Load the trained model\n",
    "model.load_state_dict(torch.load('./model_cnn.pt', map_location=torch.device('cpu')))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BiWyCjxO4zqr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5251\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose a picture at random\n",
    "idx = randint(0, test_size-1)\n",
    "print(idx)\n",
    "im, label = test_set[idx]\n",
    "org_im = inverse_transform(im)\n",
    "\n",
    "# Send to device, rescale, and view as a batch of 1 \n",
    "im = im.to(device)\n",
    "im = im.view(1,3,32,32)\n",
    "\n",
    "# Feed it to the net and display the confidence scores\n",
    "scores = model(im) \n",
    "probs = F.softmax(scores, dim=1)\n",
    "show_prob_cifar(org_im, label, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHdXjkU8oTjg"
   },
   "source": [
    "### Refining the Model\n",
    "\n",
    "Although the vallina model has achieved fair classification results, the test error is still high remaining huge room for further improvement. There are lots of methods to refine your model, such as **modifying the network architecture** (e.g., making your network deeper), **optimizing the learning strategy** (e.g., optimizer, loss function), and **tuning the hyperparameters** (e.g., learning rate, training iterations), etc. Please try at least **TWO** different methods to improve your model's performance, and discuss why these changes can work. Design and conduct your own experiments using the coding cell below, and present your experimental results and analysis in a decent way (e.g., drawing some neat figures/tables can help to convey your thoughts effectively). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "hilYigv8oTjg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleDLA(\n",
      "  (base): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (layer3): Tree(\n",
      "    (root): Root(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (left_tree): BasicBlock(\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (right_tree): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Tree(\n",
      "    (root): Root(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (left_tree): Tree(\n",
      "      (root): Root(\n",
      "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (left_tree): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (right_tree): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (right_tree): Tree(\n",
      "      (root): Root(\n",
      "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (left_tree): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (right_tree): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer5): Tree(\n",
      "    (root): Root(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (left_tree): Tree(\n",
      "      (root): Root(\n",
      "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (left_tree): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (right_tree): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (right_tree): Tree(\n",
      "      (root): Root(\n",
      "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (left_tree): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (right_tree): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer6): Tree(\n",
      "    (root): Root(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (left_tree): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (right_tree): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Conduct experiments to refine your model (e.g., modify the backbone, #\n",
    "# alter the learning strategy, and tuning the hyperparameters, etc.).        #\n",
    "##############################################################################\n",
    "\n",
    "# your code\n",
    "'''\n",
    "Deeper network, use SimpleDLA model from: https://github.com/kuangliu/pytorch-cifar/blob/master/models/dla_simple.py\n",
    "'''\n",
    "class BasicBlock(torch.nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(planes)\n",
    "        self.conv2 = torch.nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = torch.nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                torch.nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class Root(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1):\n",
    "        super(Root, self).__init__()\n",
    "        self.conv = torch.nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size,\n",
    "            stride=1, padding=(kernel_size - 1) // 2, bias=False)\n",
    "        self.bn = torch.nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        x = torch.cat(xs, 1)\n",
    "        out = F.relu(self.bn(self.conv(x)))\n",
    "        return out\n",
    "\n",
    "class Tree(torch.nn.Module):\n",
    "    def __init__(self, block, in_channels, out_channels, level=1, stride=1):\n",
    "        super(Tree, self).__init__()\n",
    "        self.root = Root(2*out_channels, out_channels)\n",
    "        if level == 1:\n",
    "            self.left_tree = block(in_channels, out_channels, stride=stride)\n",
    "            self.right_tree = block(out_channels, out_channels, stride=1)\n",
    "        else:\n",
    "            self.left_tree = Tree(block, in_channels,\n",
    "                                  out_channels, level=level-1, stride=stride)\n",
    "            self.right_tree = Tree(block, out_channels,\n",
    "                                   out_channels, level=level-1, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.left_tree(x)\n",
    "        out2 = self.right_tree(out1)\n",
    "        out = self.root([out1, out2])\n",
    "        return out\n",
    "\n",
    "\n",
    "class SimpleDLA(torch.nn.Module):\n",
    "    def __init__(self, block=BasicBlock, num_classes=10):\n",
    "        super(SimpleDLA, self).__init__()\n",
    "        self.base = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.layer3 = Tree(block,  32,  64, level=1, stride=1)\n",
    "        self.layer4 = Tree(block,  64, 128, level=2, stride=2)\n",
    "        self.layer5 = Tree(block, 128, 256, level=2, stride=2)\n",
    "        self.layer6 = Tree(block, 256, 512, level=1, stride=2)\n",
    "        self.linear = torch.nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.base(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "model_DLA = SimpleDLA()\n",
    "model_DLA.to(device)\n",
    "print(model_DLA)\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0 \t time= 0.46 min \t loss= 0.013 \t error= 59.25%\n",
      "error rate on test set = 54.17%\n",
      "epoch= 1 \t time= 0.94 min \t loss= 0.008 \t error= 35.84%\n",
      "error rate on test set = 38.06%\n",
      "epoch= 2 \t time= 1.42 min \t loss= 0.005 \t error= 24.35%\n",
      "error rate on test set = 26.02%\n",
      "epoch= 3 \t time= 1.89 min \t loss= 0.004 \t error= 18.36%\n",
      "error rate on test set = 32.51%\n",
      "epoch= 4 \t time= 2.37 min \t loss= 0.003 \t error= 14.12%\n",
      "error rate on test set = 32.08%\n",
      "epoch= 5 \t time= 2.85 min \t loss= 0.002 \t error= 10.93%\n",
      "error rate on test set = 19.47%\n",
      "epoch= 6 \t time= 3.33 min \t loss= 0.002 \t error= 8.33%\n",
      "error rate on test set = 22.81%\n",
      "epoch= 7 \t time= 3.80 min \t loss= 0.001 \t error= 6.12%\n",
      "error rate on test set = 20.52%\n",
      "epoch= 8 \t time= 4.27 min \t loss= 0.001 \t error= 4.68%\n",
      "error rate on test set = 30.05%\n",
      "epoch= 9 \t time= 4.75 min \t loss= 0.001 \t error= 3.46%\n",
      "error rate on test set = 22.17%\n",
      "epoch= 10 \t time= 5.24 min \t loss= 0.001 \t error= 2.44%\n",
      "error rate on test set = 19.18%\n",
      "epoch= 11 \t time= 5.72 min \t loss= 0.000 \t error= 2.07%\n",
      "error rate on test set = 19.48%\n",
      "epoch= 12 \t time= 6.21 min \t loss= 0.000 \t error= 1.60%\n",
      "error rate on test set = 18.73%\n",
      "epoch= 13 \t time= 6.69 min \t loss= 0.000 \t error= 1.16%\n",
      "error rate on test set = 24.41%\n",
      "epoch= 14 \t time= 7.18 min \t loss= 0.000 \t error= 0.94%\n",
      "error rate on test set = 17.92%\n",
      "epoch= 15 \t time= 7.65 min \t loss= 0.000 \t error= 0.34%\n",
      "error rate on test set = 16.18%\n",
      "epoch= 16 \t time= 8.11 min \t loss= 0.000 \t error= 0.50%\n",
      "error rate on test set = 17.08%\n",
      "epoch= 17 \t time= 8.57 min \t loss= 0.000 \t error= 0.70%\n",
      "error rate on test set = 19.20%\n",
      "epoch= 18 \t time= 9.03 min \t loss= 0.000 \t error= 0.54%\n",
      "error rate on test set = 17.73%\n",
      "epoch= 19 \t time= 9.49 min \t loss= 0.000 \t error= 0.65%\n",
      "error rate on test set = 18.19%\n",
      "epoch= 20 \t time= 9.96 min \t loss= 0.000 \t error= 0.39%\n",
      "error rate on test set = 15.87%\n",
      "epoch= 21 \t time= 10.43 min \t loss= 0.000 \t error= 0.51%\n",
      "error rate on test set = 18.28%\n",
      "epoch= 22 \t time= 10.91 min \t loss= 0.000 \t error= 0.47%\n",
      "error rate on test set = 17.74%\n",
      "epoch= 23 \t time= 11.38 min \t loss= 0.000 \t error= 0.23%\n",
      "error rate on test set = 16.90%\n",
      "epoch= 24 \t time= 11.85 min \t loss= 0.000 \t error= 0.15%\n",
      "error rate on test set = 16.02%\n",
      "epoch= 25 \t time= 12.32 min \t loss= 0.000 \t error= 0.03%\n",
      "error rate on test set = 15.95%\n",
      "epoch= 26 \t time= 12.79 min \t loss= 0.000 \t error= 0.01%\n",
      "error rate on test set = 15.33%\n",
      "epoch= 27 \t time= 13.27 min \t loss= 0.000 \t error= 0.00%\n",
      "error rate on test set = 15.03%\n",
      "epoch= 28 \t time= 13.74 min \t loss= 0.000 \t error= 0.00%\n",
      "error rate on test set = 15.11%\n",
      "epoch= 29 \t time= 14.21 min \t loss= 0.000 \t error= 0.00%\n",
      "error rate on test set = 15.10%\n",
      "epoch= 30 \t time= 14.69 min \t loss= 0.000 \t error= 0.00%\n",
      "error rate on test set = 15.23%\n",
      "epoch= 31 \t time= 15.17 min \t loss= 0.000 \t error= 0.00%\n",
      "error rate on test set = 14.93%\n",
      "epoch= 32 \t time= 15.65 min \t loss= 0.000 \t error= 0.00%\n",
      "error rate on test set = 15.02%\n",
      "epoch= 33 \t time= 16.12 min \t loss= 0.000 \t error= 0.00%\n",
      "error rate on test set = 15.04%\n",
      "epoch= 34 \t time= 16.59 min \t loss= 0.000 \t error= 0.00%\n",
      "error rate on test set = 15.12%\n",
      "epoch= 35 \t time= 17.07 min \t loss= 0.000 \t error= 0.00%\n",
      "error rate on test set = 15.05%\n",
      "epoch= 36 \t time= 17.55 min \t loss= 0.000 \t error= 0.00%\n",
      "error rate on test set = 14.97%\n",
      "epoch= 37 \t time= 18.02 min \t loss= 0.000 \t error= 0.00%\n",
      "error rate on test set = 14.95%\n",
      "epoch= 38 \t time= 18.49 min \t loss= 0.000 \t error= 0.00%\n",
      "error rate on test set = 14.85%\n",
      "epoch= 39 \t time= 18.98 min \t loss= 0.000 \t error= 0.00%\n",
      "error rate on test set = 14.96%\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Conduct experiments to refine your model (e.g., modify the backbone, #\n",
    "# alter the learning strategy, and tuning the hyperparameters, etc.).        #\n",
    "##############################################################################\n",
    "\n",
    "# your code\n",
    "\n",
    "'''\n",
    "Train more epochs\n",
    "'''\n",
    "dla_criterion = torch.nn.CrossEntropyLoss()\n",
    "dla_optimizer = optim.SGD(model_DLA.parameters(),lr=learning_rate)\n",
    "dla_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(dla_optimizer, T_max=200)\n",
    "\n",
    "def train_more_epochs(model,num_epochs = epochs):\n",
    "    start=time.time()\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # set the running quatities to zero at the beginning of the epoch\n",
    "        running_loss = 0\n",
    "        running_error = 0\n",
    "    \n",
    "        for data in train_loader:\n",
    "            pass\n",
    "            ##############################################################################\n",
    "            # TODO: Implement the training process.                                      #\n",
    "            ##############################################################################\n",
    "\n",
    "            # Load inputs and labels and deploy to running device\n",
    "            tmp_img, tmp_label = data\n",
    "            tmp_img, tmp_label = tmp_img.cuda(), tmp_label.cuda()\n",
    "\n",
    "            # Set the gradients to zeros\n",
    "            dla_optimizer.zero_grad()\n",
    "            # Forward the batch data through the net\n",
    "            tmp_output = model(tmp_img)    \n",
    "\n",
    "            # Compute the average of the losses of the data points in the minibatch\n",
    "            tmp_loss = dla_criterion(tmp_output, tmp_label)\n",
    "\n",
    "            # Backward pass to compute gradients\n",
    "            tmp_loss.backward()\n",
    "\n",
    "            # Do one step of stochastic gradient descent\n",
    "            dla_optimizer.step()\n",
    "\n",
    "            # Add the loss of this batch to the running loss\n",
    "            running_loss += tmp_loss.item()\n",
    "\n",
    "            # Compute the error made on this batch and add it to the running error\n",
    "            _, tmp_predicted = torch.max(tmp_output, 1)\n",
    "            running_error += (tmp_predicted != tmp_label).sum().item()\n",
    "            \n",
    "            ##############################################################################\n",
    "            #                             END OF YOUR CODE                               #\n",
    "            ##############################################################################\n",
    "            \n",
    "        # Compute stats for the full training set\n",
    "        total_loss = running_loss / train_size\n",
    "        total_error = running_error / train_size\n",
    "        elapsed = (time.time()-start) / 60\n",
    "        \n",
    "        print('epoch= {} \\t time= {:.2f} min \\t loss= {:.3f} \\t error= {:.2f}%'.format(epoch, elapsed, total_loss, total_error * 100))\n",
    "        eval_on_test_set(model)\n",
    "        dla_scheduler.step()\n",
    "\n",
    "\n",
    "# Start training\n",
    "train_more_epochs(model_DLA,40)\n",
    "# Save the trained model \n",
    "torch.save(model.state_dict(), './model_DLA.pt')\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-03-04 15:35:08 (running for 00:00:05.73)\n",
      "Memory usage on this node: 14.1/31.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/32 CPUs, 0/1 GPUs, 0.0/12.93 GiB heap, 0.0/6.46 GiB objects\n",
      "Result logdir: /home/lwh/Documents/Code/Msc_course/EE6934/Project2/ray_result/DL_Proj2_task1\n",
      "Number of trials: 10/10 (10 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2023-03-04 15:35:13 (running for 00:00:10.74)\n",
      "Memory usage on this node: 14.1/31.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/32 CPUs, 0/1 GPUs, 0.0/12.93 GiB heap, 0.0/6.46 GiB objects\n",
      "Result logdir: /home/lwh/Documents/Code/Msc_course/EE6934/Project2/ray_result/DL_Proj2_task1\n",
      "Number of trials: 10/10 (10 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2023-03-04 15:35:18 (running for 00:00:15.74)\n",
      "Memory usage on this node: 14.0/31.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/32 CPUs, 0/1 GPUs, 0.0/12.93 GiB heap, 0.0/6.46 GiB objects\n",
      "Result logdir: /home/lwh/Documents/Code/Msc_course/EE6934/Project2/ray_result/DL_Proj2_task1\n",
      "Number of trials: 10/10 (10 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2023-03-04 15:35:23 (running for 00:00:20.74)\n",
      "Memory usage on this node: 14.1/31.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/32 CPUs, 0/1 GPUs, 0.0/12.93 GiB heap, 0.0/6.46 GiB objects\n",
      "Result logdir: /home/lwh/Documents/Code/Msc_course/EE6934/Project2/ray_result/DL_Proj2_task1\n",
      "Number of trials: 10/10 (10 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2023-03-04 15:35:28 (running for 00:00:25.75)\n",
      "Memory usage on this node: 14.0/31.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/32 CPUs, 0/1 GPUs, 0.0/12.93 GiB heap, 0.0/6.46 GiB objects\n",
      "Result logdir: /home/lwh/Documents/Code/Msc_course/EE6934/Project2/ray_result/DL_Proj2_task1\n",
      "Number of trials: 10/10 (10 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2023-03-04 15:35:33 (running for 00:00:30.75)\n",
      "Memory usage on this node: 14.2/31.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/32 CPUs, 0/1 GPUs, 0.0/12.93 GiB heap, 0.0/6.46 GiB objects\n",
      "Result logdir: /home/lwh/Documents/Code/Msc_course/EE6934/Project2/ray_result/DL_Proj2_task1\n",
      "Number of trials: 10/10 (10 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2023-03-04 15:35:38 (running for 00:00:35.76)\n",
      "Memory usage on this node: 14.1/31.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/32 CPUs, 0/1 GPUs, 0.0/12.93 GiB heap, 0.0/6.46 GiB objects\n",
      "Result logdir: /home/lwh/Documents/Code/Msc_course/EE6934/Project2/ray_result/DL_Proj2_task1\n",
      "Number of trials: 10/10 (10 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2023-03-04 15:35:43 (running for 00:00:40.76)\n",
      "Memory usage on this node: 14.0/31.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/32 CPUs, 0/1 GPUs, 0.0/12.93 GiB heap, 0.0/6.46 GiB objects\n",
      "Result logdir: /home/lwh/Documents/Code/Msc_course/EE6934/Project2/ray_result/DL_Proj2_task1\n",
      "Number of trials: 10/10 (10 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2023-03-04 15:35:48 (running for 00:00:45.77)\n",
      "Memory usage on this node: 14.0/31.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/32 CPUs, 0/1 GPUs, 0.0/12.93 GiB heap, 0.0/6.46 GiB objects\n",
      "Result logdir: /home/lwh/Documents/Code/Msc_course/EE6934/Project2/ray_result/DL_Proj2_task1\n",
      "Number of trials: 10/10 (10 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2023-03-04 15:35:53 (running for 00:00:50.77)\n",
      "Memory usage on this node: 14.1/31.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/32 CPUs, 0/1 GPUs, 0.0/12.93 GiB heap, 0.0/6.46 GiB objects\n",
      "Result logdir: /home/lwh/Documents/Code/Msc_course/EE6934/Project2/ray_result/DL_Proj2_task1\n",
      "Number of trials: 10/10 (10 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2023-03-04 15:35:58 (running for 00:00:55.77)\n",
      "Memory usage on this node: 14.1/31.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/32 CPUs, 0/1 GPUs, 0.0/12.93 GiB heap, 0.0/6.46 GiB objects\n",
      "Result logdir: /home/lwh/Documents/Code/Msc_course/EE6934/Project2/ray_result/DL_Proj2_task1\n",
      "Number of trials: 10/10 (10 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2023-03-04 15:36:03 (running for 00:01:00.78)\n",
      "Memory usage on this node: 14.0/31.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/32 CPUs, 0/1 GPUs, 0.0/12.93 GiB heap, 0.0/6.46 GiB objects\n",
      "Result logdir: /home/lwh/Documents/Code/Msc_course/EE6934/Project2/ray_result/DL_Proj2_task1\n",
      "Number of trials: 10/10 (10 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2023-03-04 15:36:08 (running for 00:01:05.78)\n",
      "Memory usage on this node: 14.0/31.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/32 CPUs, 0/1 GPUs, 0.0/12.93 GiB heap, 0.0/6.46 GiB objects\n",
      "Result logdir: /home/lwh/Documents/Code/Msc_course/EE6934/Project2/ray_result/DL_Proj2_task1\n",
      "Number of trials: 10/10 (10 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2023-03-04 15:36:13 (running for 00:01:10.78)\n",
      "Memory usage on this node: 14.2/31.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/32 CPUs, 0/1 GPUs, 0.0/12.93 GiB heap, 0.0/6.46 GiB objects\n",
      "Result logdir: /home/lwh/Documents/Code/Msc_course/EE6934/Project2/ray_result/DL_Proj2_task1\n",
      "Number of trials: 10/10 (10 PENDING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 15:36:14,303\tWARNING tune.py:146 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2023-03-04 15:36:18,330\tERROR tune.py:794 -- Trials did not complete: [hyper_train_1332e_00000, hyper_train_1332e_00001, hyper_train_1332e_00002, hyper_train_1332e_00003, hyper_train_1332e_00004, hyper_train_1332e_00005, hyper_train_1332e_00006, hyper_train_1332e_00007, hyper_train_1332e_00008, hyper_train_1332e_00009]\n",
      "2023-03-04 15:36:18,332\tINFO tune.py:798 -- Total run time: 76.01 seconds (75.80 seconds for the tuning loop).\n",
      "2023-03-04 15:36:18,333\tWARNING tune.py:804 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n",
      "2023-03-04 15:36:18,337\tWARNING experiment_analysis.py:621 -- Could not find best trial. Did you pass the correct `metric` parameter?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-03-04 15:36:18 (running for 00:01:15.80)\n",
      "Memory usage on this node: 14.2/31.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/32 CPUs, 0/1 GPUs, 0.0/12.93 GiB heap, 0.0/6.46 GiB objects\n",
      "Result logdir: /home/lwh/Documents/Code/Msc_course/EE6934/Project2/ray_result/DL_Proj2_task1\n",
      "Number of trials: 10/10 (10 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2023-03-04 15:36:18 (running for 00:01:15.80)\n",
      "Memory usage on this node: 14.2/31.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/32 CPUs, 0/1 GPUs, 0.0/12.93 GiB heap, 0.0/6.46 GiB objects\n",
      "Result logdir: /home/lwh/Documents/Code/Msc_course/EE6934/Project2/ray_result/DL_Proj2_task1\n",
      "Number of trials: 10/10 (10 PENDING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-afb062fc0dcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0meval_on_test_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m \u001b[0mtorch_ray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;31m##############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;31m#                             END OF YOUR CODE                               #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-afb062fc0dcb>\u001b[0m in \u001b[0;36mtorch_ray\u001b[0;34m(num_samples, max_epochs)\u001b[0m\n\u001b[1;32m    144\u001b[0m     )\n\u001b[1;32m    145\u001b[0m     \u001b[0mbest_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"min\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"last\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best trial config: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best trial final validation loss: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'config'"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Conduct experiments to refine your model (e.g., modify the backbone, #\n",
    "# alter the learning strategy, and tuning the hyperparameters, etc.).        #\n",
    "##############################################################################\n",
    "\n",
    "# your code\n",
    "'''\n",
    "Hyper paramenter tuning. Using ray tune\n",
    "Vary learining rate. Uniform distribution, [1e-4, 3e-1]\n",
    "'''\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.air import RunConfig\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# Define searching space\n",
    "config = {\"lr\":tune.loguniform(1e-4,3e-1)}\n",
    "# Ray result folder\n",
    "run_config = RunConfig(\n",
    "    name = \"DL_Proj2_task1\",\n",
    "    local_dir = \"ray_result\",\n",
    "    verbose = 1,\n",
    ")\n",
    "\n",
    "def load_data(data_dir=\"./data\"):\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=CIFAR_MEAN.tolist(), std=CIFAR_STD.tolist())\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=True, download=True, transform=transform)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "    return trainset, testset\n",
    "\n",
    "\n",
    "# Define train function which will used in ray tune\n",
    "def hyper_train(config,data_dir=None):\n",
    "    model_lr = ConvNet()\n",
    "    model_lr.to(device)\n",
    "    trainset, testset = load_data(data_dir)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "    dataloaders = {\"train\": trainloader, \"test\": testloader}\n",
    "    \n",
    "    hpyer_criterion = torch.nn.CrossEntropyLoss()\n",
    "    hyper_optimizer = optim.SGD(model_lr.parameters(),lr=config[\"lr\"])\n",
    "\n",
    "    start=time.time()\n",
    "    model_lr.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # set the running quatities to zero at the beginning of the epoch\n",
    "        running_loss = 0\n",
    "        running_error = 0\n",
    "    \n",
    "        for data in trainloader:\n",
    "            pass\n",
    "            ##############################################################################\n",
    "            # TODO: Implement the training process.                                      #\n",
    "            ##############################################################################\n",
    "\n",
    "            # Load inputs and labels and deploy to running device\n",
    "            tmp_img, tmp_label = data\n",
    "            tmp_img, tmp_label = tmp_img.cuda(), tmp_label.cuda()\n",
    "\n",
    "            # Set the gradients to zeros\n",
    "            hyper_optimizer.zero_grad()\n",
    "            # Forward the batch data through the net\n",
    "            tmp_output = model_lr(tmp_img)    \n",
    "\n",
    "            # Compute the average of the losses of the data points in the minibatch\n",
    "            tmp_loss = hpyer_criterion(tmp_output, tmp_label)\n",
    "\n",
    "            # Backward pass to compute gradients\n",
    "            tmp_loss.backward()\n",
    "\n",
    "            # Do one step of stochastic gradient descent\n",
    "            hyper_optimizer.step()\n",
    "\n",
    "            # Add the loss of this batch to the running loss\n",
    "            running_loss += tmp_loss.item()\n",
    "\n",
    "            # Compute the error made on this batch and add it to the running error\n",
    "            _, tmp_predicted = torch.max(tmp_output, 1)\n",
    "            running_error += (tmp_predicted != tmp_label).sum().item()\n",
    "            \n",
    "            ##############################################################################\n",
    "            #                             END OF YOUR CODE                               #\n",
    "            ##############################################################################\n",
    "            \n",
    "        # Compute stats for the full training set\n",
    "        total_loss = running_loss / train_size\n",
    "        total_error = running_error / train_size\n",
    "        elapsed = (time.time()-start) / 60\n",
    "        \n",
    "        print('epoch= {} \\t time= {:.2f} min \\t loss= {:.3f} \\t error= {:.2f}%'.format(epoch, elapsed, total_loss, total_error * 100))\n",
    "        eval_on_test_set(model_lr)\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save(model_lr.state_dict(),hyper_optimizer.state_dict(),path)\n",
    "        tune.report(loss=total_loss,error=total_error)\n",
    "\n",
    "\n",
    "def torch_ray(num_samples=10, max_epochs=20):\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=max_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2\n",
    "    )\n",
    "    data_dir = os.path.abspath(\"./data\")\n",
    "    # result = tune.Tuner(\n",
    "    #     hyper_train,\n",
    "    #     param_space={\n",
    "    #         \"train_loop_config\":config,\n",
    "    #         \"num_workers\":1,\n",
    "    #         \"num_cpus_per_worker\":6,\n",
    "    #         \"num_gpus_per_worker\":1,\n",
    "    #     },\n",
    "    #     run_config = run_config\n",
    "\n",
    "    # )\n",
    "    reporter = CLIReporter(\n",
    "        metric_columns=[\"loss\",\"error\",\"training_iteration\"]\n",
    "    )\n",
    "    result = tune.run(\n",
    "        partial(hyper_train, data_dir=data_dir),\n",
    "        name=\"DL_Proj2_task1\",\n",
    "        local_dir=\"ray_result\",\n",
    "        verbose=1,\n",
    "        resources_per_trial={\"cpu\":8,\"gpu\":1},\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter,\n",
    "    )\n",
    "    best_trial = result.get_best_trial(\"loss\",\"min\",\"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(best_trial.last_result[\"loss\"]))\n",
    "    best_model = ConvNet()\n",
    "    best_model.to(device)\n",
    "    best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    model_state, optimizer_state = torch.load(os.path.join(best_checkpoint_dir, \"checkpoint\"))\n",
    "    best_model.load_state_dict(model_state)\n",
    "    eval_on_test_set(best_model)\n",
    "\n",
    "torch_ray()\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGKGlsYMoTjg"
   },
   "source": [
    "---\n",
    "\n",
    "**Write your observations and analysis in this Markdown cell:**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dQFsHMr_8ol"
   },
   "source": [
    "## Task 2: Exploring Model Training\n",
    "\n",
    "The model is doing well, eating images and predicting results; however, everything that happens inside is opaque and hard to explain. So, how can we interpret how CNN sees and understands when making a decision? \n",
    "\n",
    "In this section, we are going to explore Grad-CAM, a visual explanation algorithm that generates heatmaps indicating where the network is \"looking\" in the forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uk_pbk_goTjg"
   },
   "source": [
    "### Grad-CAM\n",
    "\n",
    "Gradient-weighted Class Activation Mapping (Grad-CAM) [1], uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. An overview of the workflow of Grad-CAM is shown below. Please read the paper [1], understand the algorithm, and implement `generate_cam`.\n",
    "\n",
    "[1] [Selvaraju, Ramprasaath R., et al. \"Grad-cam: Visual explanations from deep networks via gradient-based localization.\" ICCV 2017.](https://arxiv.org/abs/1610.02391)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VFgVaE-YoTjh"
   },
   "outputs": [],
   "source": [
    "gradients = []  # A gloabl variable used to save the gradient\n",
    "def generate_cam(model, input_image, target_layer='conv4a', target_class=None):\n",
    "    \"\"\"\n",
    "    A function to generate Grad-CAM of specific layer and class on an input image using given model.\n",
    "  \n",
    "    Inputs\n",
    "    - model: A PyTorch model.\n",
    "    - input_image: A PyTorch Tensor of shape (1, C, H, W).\n",
    "    - target_layer: A String indicating the name of targeted convolutional layer being visualized (e.g., 'conv4').\n",
    "                    By default, use the last conv layer of the model.\n",
    "    - target_class: An Integer indicating the lable of targeted class being visualized (e.g., 1). \n",
    "                    If None, use the predicted class as target class.\n",
    "    \n",
    "    Returns: A NumPy Array of shape (N, C, H, W) showing the intended heatmap.\n",
    "    \"\"\"\n",
    "\n",
    "    ##############################################################################\n",
    "    # TODO: Given an input image, generate its Grad-CAM on target conv layer     #\n",
    "    # using the backward gradients from a specific class.                        #\n",
    "    # 1. Forward the input image, when you also need to register the gradient    #\n",
    "    # hook so as to get the gradient in backward pass (hint: register_hook).     #\n",
    "    # 2. Backward pass with specified target class, and get gradients.           #\n",
    "    # 3. Average each gradient, multiply with its conv output, and sum together. #\n",
    "    ##############################################################################\n",
    "    \n",
    "    # your code\n",
    "\n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "\n",
    "    # Post processing\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = (cam - np.min(cam)) / (np.max(cam) - np.min(cam))  # Normalize between 0-1\n",
    "    cam = np.uint8(cam * 255)  # Scale between 0-255 to visualize\n",
    "    cam = np.uint8(Image.fromarray(cam).resize((input_image.shape[2],\n",
    "                    input_image.shape[3]), Image.ANTIALIAS))\n",
    "\n",
    "    return cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rbyecfmLoTjh"
   },
   "outputs": [],
   "source": [
    "idx = randint(0, test_size-1)\n",
    "im, label = test_set[idx]\n",
    "org_im = inverse_transform(im)\n",
    "\n",
    "im = im.to(device)\n",
    "im = im.view(1,3,32,32)\n",
    "\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.imshow(org_im)\n",
    "ax.set_title('Original Image')\n",
    "ax.set_axis_off()\n",
    "\n",
    "cam = generate_cam(model, im)\n",
    "heatmap, heatmap_on_image = apply_colormap_on_image(org_im, cam, 'hsv')\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.imshow(heatmap_on_image)\n",
    "ax.set_title('Grad-CAM')\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nXnIUoeoTjh"
   },
   "source": [
    "### Performing Visual Explanation\n",
    "\n",
    "Now you have successfully built a powerful tool (i.e., Grad-CAM) that can assist you to visualize and understand the CNN models. Try to make full use of `generate_cam` and design at least **TWO** experiments that can further explain how CNN sees and understands the images. For example, apply Grad-CAM on different layers and compare their differences. Please quantitatively show some evidence (e.g., plotting some examplar images clearly and elegantly) with necessary code snippets, write down your observations and briefly explain each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvGGXCHSoTjh"
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Write code snippets to present your experiments (hint: if you want   #\n",
    "# to plot multiple images in one single figure, plt.subplot should help).    #\n",
    "##############################################################################\n",
    "\n",
    "# your code\n",
    "\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSG_WIvdoTjh"
   },
   "source": [
    "---\n",
    "\n",
    "**Write your observations and analysis in this Markdown cell:**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HdsDUNRBZUO"
   },
   "source": [
    "## Task 3: Adversarial Attack\n",
    "\n",
    "After going through some testing results in Task 1, you might think the model performs pretty well, predicting the correct classes with high confidence, and rarely making mistakes. However, the network is not as strong as you think. The network can be vulnerable and easily fooled by simply adding some very small distributions on input images. In this section, we are going to attack our previously trained model by generating adversarial images that visually look alike but crush the model without striking a blowing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgWZORu2oTjh"
   },
   "source": [
    "### Adversarial Examples\n",
    "\n",
    "Adversarial examples are specialised inputs created with the purpose of confusing a neural network, resulting in the misclassification of a given input. These examples are usually generated by adding imperceptible non-random perturbations to the image, which are indistinguishable to human eye, but can cause the network to fail to identify the contents of the image. There are many kinds of such attack strategies, here we focus on the Fast Gradient Sign Method (FGSM)[2]. FGSM is a white box attack where the attacker has complete access to the model being attacked. The method uses the gradients of the loss with respect to the input image to create a new image that maximises the loss. Below shows a famous exmaple taken from the paper [2], where the classification of image showing \"panda\" is changed to \"gibbon\" after adding an imperceptibly small vector: $\\text{sign}(\\nabla_xJ(\\theta, x, y))$.\n",
    "\n",
    "Kurakin et al.[3] propose an iterative FGSM (I-FGSM) attack by repeating FGSM for $n$ steps. I-FSGM usually results in higher classification error than FGSM. Read the papers below [2, 3], and  implement the I-FGSM within `ifgsm_attack`.\n",
    "\n",
    "[2] [Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. \"Explaining and harnessing adversarial examples.\" ICLR 2015.](https://arxiv.org/abs/1412.6572) \\\n",
    "[3] [Kurakin, Alexey, Ian Goodfellow, and Samy Bengio. \"Adversarial machine learning at scale.\" ICLR 2017.](https://arxiv.org/pdf/1611.01236)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "774Gs5AroTjh"
   },
   "source": [
    "### Attacking CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y1a0g1QAoTjh"
   },
   "outputs": [],
   "source": [
    "def ifgsm_attack(model, images, labels, targets=None, eps=0.07, steps=10):\n",
    "    \"\"\"\n",
    "    Iterative Fast Gradient Sign Method (I-FGSM), https://arxiv.org/pdf/1611.01236\n",
    "  \n",
    "    Arguments:\n",
    "        - model (nn.Module): model to attack.\n",
    "        - images (torch.Tensor): input images, shape (N, C, H, W).\n",
    "        - labels (torch.Tensor): ground truth labels of input images, shape (N,).\n",
    "        - targets (torch.Tensor): the target classes you want the model to misclassify, shape (N,).\n",
    "        - eps (float): maximum perturbation. \n",
    "        - steps (int): number of iterations.\n",
    "\n",
    "    Output: The adversarial examples of input images.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    ##############################################################################\n",
    "    # TODO: Implement Fast Gradient Sign Method to generate an adversarial image #\n",
    "    # that fools the model to predict incorrect class.                           #   \n",
    "    # 1. Get the gradients of the loss w.r.t to the input image.                 #\n",
    "    # 2. Get the sign of the gradients to create the perturbation.               #\n",
    "    # 3. Add the perturbation to the input image iteratively for n steps.        #\n",
    "    ##############################################################################\n",
    "    \n",
    "    # your code\n",
    "\n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "\n",
    "    return adv_images.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vdjhrjaoTjh"
   },
   "source": [
    "Pick up some images from test set, and see if the attack is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUQ5wHVxoTjh"
   },
   "outputs": [],
   "source": [
    "idx = randint(0, len(test_set)-1)\n",
    "im, label = test_set[idx]\n",
    "org_im = inverse_transform(im)\n",
    "\n",
    "im = im.to(device)\n",
    "im = im.view(1,3,32,32)\n",
    "\n",
    "print('Before attack:')\n",
    "scores = model(im) \n",
    "probs = F.softmax(scores, dim=1)\n",
    "show_prob_cifar(org_im, label, probs)\n",
    "\n",
    "adv_im = ifgsm_attack(model, im, label)\n",
    "\n",
    "print('After attack:')\n",
    "scores = model(adv_im) \n",
    "probs = F.softmax(scores, dim=1)\n",
    "show_prob_cifar(inverse_transform(adv_im[0]), label, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvBCCgCloTjh"
   },
   "source": [
    "Run evaluation on the testing set to see how the model performs on the generated adversarial examples, and compare with results before attacks. Try different parameters (e.g., `targets`, `eps`), describe your observations with a brief explanation in the below cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2yLv3j0oTjh"
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Evaluate error rate on adversarial images generated from testing set #\n",
    "# with different parameter settings.                                         #\n",
    "##############################################################################\n",
    "\n",
    "# your code\n",
    "\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpJroP7GoTji"
   },
   "source": [
    "---\n",
    "\n",
    "**Write your observations and analysis in this Markdown cell:**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
